{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# load data: \n",
    "data = pd.read_csv('1_R/ML_7111genes.csv',index_col=0)\n",
    "print(data.shape)\n",
    "\n",
    "gene_names = data.drop(['cohort','label'],axis=1).columns\n",
    "print(len(gene_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data = data[data['label'].isin(['DN', 'HCC','Y'])]\n",
    "print(data.shape)\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "enc = LabelEncoder()   # LabelEncoder DM,HCC,Y\n",
    "data['label'] = enc.fit_transform(data['label'])   \n",
    "\n",
    "inter = data.loc[data['cohort']=='inter']\n",
    "X_inter = inter.drop(['label','cohort'],axis=1).astype(np.float64)\n",
    "y_inter = inter['label']\n",
    "\n",
    "exter = data.loc[data['cohort']=='exter']\n",
    "X_exterTest = exter.drop(['label','cohort'],axis=1).astype(np.float64)\n",
    "y_exterTest = exter['label']\n",
    "\n",
    "print(y_inter.shape, y_exterTest.shape)\n",
    "print(X_inter.shape, X_exterTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. screen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# record the importance score and then sum.\n",
    "importance_scores = np.zeros((X_inter.shape[1],))\n",
    "for seed in range(10):  # 10 times-5 fold\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    for train_index, test_index in kf.split(X_inter): # 1/5\n",
    "        X_train = X_inter.iloc[train_index]\n",
    "        y_train = y_inter.iloc[train_index]\n",
    "        \n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "\n",
    "        # screen features\n",
    "        clf = OneVsOneClassifier(xgb.XGBClassifier(random_state=42))\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        for estimator in clf.estimators_:\n",
    "            importance_scores += estimator.feature_importances_  # importance \n",
    "\n",
    "# mean \n",
    "average_importance_scores = importance_scores / (10* 5)  # total\n",
    "\n",
    "# top 10 \n",
    "top10_idx = np.argsort(average_importance_scores)[-10:][::-1]  # top 10 index\n",
    "top10_genes = gene_names[top10_idx]  # gene name\n",
    "top10_scores = average_importance_scores[top10_idx]  # importance score\n",
    "\n",
    "gene_importance_dict = dict(zip(top10_genes, top10_scores))\n",
    "\n",
    "with open('2_RNAmodel_vexter/gene_importance-sumscore.json', 'w') as json_file:\n",
    "    json.dump(gene_importance_dict, json_file)\n",
    "\n",
    "print(\"Top 10 :\")\n",
    "for gene, score in zip(top10_genes, top10_scores):\n",
    "    print(f\"{gene}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. visualization: importance score heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import json\n",
    "\n",
    "with open('gene_importance-sumscore.json', 'r') as json_file:\n",
    "    gene_importance_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "gene_importance = dict(sorted(gene_importance_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "top10_genes = list(gene_importance.keys())[:10]\n",
    "\n",
    "scores = np.array([gene_importance_dict[gene] for gene in top10_genes])\n",
    "scores = (scores - np.mean(scores)) / np.std(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "scores[0] = 1 # max value\n",
    "top10_score_array = scores.reshape(-1, 1)\n",
    "\n",
    "plt.imshow(top10_score_array, cmap='OrRd', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks(ticks=np.arange(len(top10_genes)), labels=top10_genes, \n",
    "        rotation=45, ha='right', fontdict={'fontstyle': 'italic'})\n",
    "\n",
    "plt.title('Gene Importance Scores')\n",
    "\n",
    "plt.savefig('importance_score.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperparameter traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "with open('2_RNAmodel_vexter/geneSumscore/gene_importance-sumscore.json', 'r') as json_file:\n",
    "    gene_importance = json.load(json_file)\n",
    "\n",
    "top10_genes = gene_importance.keys()\n",
    "top10_indices = [list(gene_names).index(top10) for top10 in top10_genes]\n",
    "\n",
    "\n",
    "X_inter2 = X_inter.iloc[:, top10_indices] \n",
    "print(X_inter2.shape)\n",
    "\n",
    "X_exterTest2 = X_exterTest.iloc[:, top10_indices] \n",
    "print(X_exterTest2.shape)\n",
    "\n",
    "# save\n",
    "with open(f'2_RNAmodel_vexter/geneSumscore/data_inter-exter.pkl', 'wb') as f:\n",
    "    pickle.dump((X_inter2, y_inter, X_exterTest2, y_exterTest), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "with open(f'geneSumscore/data_inter-exter.pkl', 'rb') as f:\n",
    "    X_inter2, y_inter, X_exterTest2, y_exterTest  = pickle.load(f)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth':range(1,3,1),\n",
    "    'n_estimators': np.arange(10,50,10),\n",
    "    'learning_rate': [i/10.0 for i in range(1, 5, 2)],\n",
    "    \n",
    "    # 'min_child_weight':[3,4,5],\n",
    "    # 'gamma':[i/10.0 for i in range(1, 9, 2)],\n",
    "    # 'subsample':[i/10.0 for i in range(1, 9, 2)],\n",
    "    # 'colsample_bytree':[i/10.0 for i in range(1, 9, 2)],\n",
    "}\n",
    "\n",
    "param_values = [param_grid[param] for param in param_grid]\n",
    "param_combinations = list(itertools.product(*param_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for idx, combination in enumerate(param_combinations): # every hyperparameter combination\n",
    "    param = dict(zip(param_grid.keys(), combination))\n",
    "    \n",
    "    y_pred_all, y_pred_proba_all, y_true_all = [],[],[]\n",
    "    \n",
    "    for seed in range(10):  # 10 times - 5 folds\n",
    "        \n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        y_pred, y_pred_proba, y_true = [],[],[]\n",
    "\n",
    "        for train_index, test_index in kf.split(X_inter): # 1/5\n",
    "            X_train, X_test = X_inter.iloc[train_index], X_inter.iloc[test_index]\n",
    "            y_train, y_test = y_inter.iloc[train_index], y_inter.iloc[test_index]\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            # modeling\n",
    "            clf = OneVsOneClassifier(xgb.XGBClassifier(**param))\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # pred\n",
    "            y_pred.extend(clf.predict(X_test)) \n",
    "            y_true.extend(y_test)\n",
    "\n",
    "        y_pred_all.append(y_pred)\n",
    "        y_true_all.append(y_true)\n",
    "    \n",
    "    y_pred_all = np.array(y_pred_all).flatten()\n",
    "    y_true_all = np.array(y_true_all).flatten()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Pred': y_pred_all,\n",
    "        'True': y_true_all\n",
    "    })\n",
    "\n",
    "    df.to_csv(f'2_RNAmodel_vexter/geneSumscore/Hyperparameters/HyperCombination_{idx}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hyperparameter evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx    5.000000\n",
      "acc    0.703077\n",
      "f1     0.706936\n",
      "Name: 5, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "dfList = glob.glob('HyperCombination_*.csv')\n",
    "accAll, f1All = [],[]\n",
    "for dfPath in dfList:\n",
    "    df = pd.read_csv(dfPath)\n",
    "    y_pred = df['Pred']\n",
    "    y_true = df['True']\n",
    "\n",
    "    accAll.append(accuracy_score(y_true, y_pred))\n",
    "    f1All.append(f1_score(y_true, y_pred, average='weighted'))\n",
    "    \n",
    "mtrx = pd.DataFrame({'idx':range(16),\n",
    "            'acc': accAll,\n",
    "            'f1': f1All\n",
    "            })\n",
    "\n",
    "new_mtrx = mtrx.sort_values(by='acc', ascending=False)\n",
    "print(new_mtrx.iloc[0]) \n",
    "top_idx = new_mtrx.iloc[0,0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30, 0.3)\n"
     ]
    }
   ],
   "source": [
    "print(param_combinations[top_idx])\n",
    "# param= {'max_depth':1,\n",
    "#         'n_estimators': 30,\n",
    "#         'learning_rate': 0.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calulate matics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 4)\n"
     ]
    }
   ],
   "source": [
    "cohort = 'exterTest' # exterTest\n",
    "dfPath = f'evaluation/evaluation_{cohort}.csv'  \n",
    "df = pd.read_csv(dfPath)\n",
    "print(df.shape)\n",
    "\n",
    "# 2 classes\n",
    "df2 = df.loc[(df['Pred']!= 2)& (df['True']!= 2)]  \n",
    "\n",
    "accList, preList, recallList, f1List = [],[],[],[]\n",
    "for idx in range(10):\n",
    "\n",
    "    df_plot = df2[df2['Seed'] == idx]\n",
    "    y_true = df_plot['True']\n",
    "    y_pred = df_plot['Pred']\n",
    "    y_prob = df_plot['HCCprob']\n",
    "\n",
    "    accList.append(accuracy_score(y_true, y_pred))\n",
    "    preList.append(precision_score(y_true, y_pred))\n",
    "    recallList.append(recall_score(y_true, y_pred))\n",
    "    f1List.append(f1_score(y_true, y_pred))\n",
    "\n",
    "mtrx = pd.DataFrame({\n",
    "    'acc': accList,\n",
    "    'precision':preList,\n",
    "    'recall':recallList,\n",
    "    'f1':f1List\n",
    "})\n",
    "mtrx.to_csv(f'evaluation/mtrx_{cohort}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy is {mtrx[\"acc\"].mean(),mtrx[\"acc\"].std()}')\n",
    "print(f'Precision is {mtrx[\"precision\"].mean(),mtrx[\"precision\"].std()}')\n",
    "print(f'Recall is {mtrx[\"recall\"].mean(),mtrx[\"recall\"].std()}')\n",
    "print(f'F1 is {mtrx[\"f1\"].mean(),mtrx[\"f1\"].std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# external test set：\n",
    "dfPath = f'evaluation/evaluation_exterTest.csv' \n",
    "df = pd.read_csv(dfPath)\n",
    "print(df.shape)\n",
    "\n",
    "df_plot = df.loc[(df['Pred']!= 2)& (df['True']!= 2)]  # 注意2是Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "y_true = df_plot['True']\n",
    "y_pred = df_plot['Pred']\n",
    "y_prob = df_plot['HCCprob']\n",
    "\n",
    "print(f'Accuracy is {accuracy_score(y_true, y_pred)}')\n",
    "print(f'Precision is {precision_score(y_true, y_pred)}')\n",
    "print(f'Recall is {recall_score(y_true, y_pred)}')\n",
    "print(f'F1 is {f1_score(y_true, y_pred)}')\n",
    "print(f'AUC is {roc_auc_score(y_true, y_prob)}')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix= cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC \n",
    "DN & HCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import auc, roc_curve, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 4)\n"
     ]
    }
   ],
   "source": [
    "dfPath = 'geneSumscore/evaluation/evaluation_exterTest.csv' # inter_5fold, \n",
    "df = pd.read_csv(dfPath)\n",
    "print(df.shape)\n",
    "\n",
    "df_plot = df.loc[(df['Pred']!= 2)& (df['True']!= 2)]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(df_plot['True'], df_plot['HCCprob'])\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 4)\n"
     ]
    }
   ],
   "source": [
    "cohort = 'exterTest'   # inter_5fold, exterTest\n",
    "dfPath = f'geneSumscore/evaluation/evaluation_{cohort}.csv' # inter_5fold, exterTest\n",
    "df = pd.read_csv(dfPath)\n",
    "print(df.shape)\n",
    "\n",
    "roc_curves = []\n",
    "auc_values = []\n",
    "\n",
    "for i in range(10): # load results\n",
    "    df_seed = df.loc[df['Seed'] == i,:]\n",
    "    df_plot = df_seed.loc[(df['Pred']!= 2)& (df['True']!= 2)]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(df_plot['True'], df_plot['HCCprob'])\n",
    "    roc_curves.append((fpr, tpr))\n",
    "    auc_values.append(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.mean([np.interp(mean_fpr, curve[0], curve[1]) for curve in roc_curves], axis=0)\n",
    "mean_auc = np.mean(auc_values)\n",
    "std_auc = np.std(auc_values)\n",
    "print(mean_auc, std_auc)\n",
    "\n",
    "std_tpr = np.std([np.interp(mean_fpr, curve[0], curve[1]) for curve in roc_curves], axis=0)\n",
    "\n",
    "# draw ROC\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(mean_fpr, mean_tpr, color='red', label='AUROC = {:.2f}'.format(mean_auc))\n",
    "plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, color='#FF7F7F', alpha=0.5)\n",
    "\n",
    "# draw others\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.savefig(f'geneSumscore/evaluation/ROC_{cohort}.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 4)\n"
     ]
    }
   ],
   "source": [
    "dfPath = 'geneSumscore/evaluation/evaluation_exterTest.csv' # exterTest, inter_5fold\n",
    "df = pd.read_csv(dfPath)\n",
    "print(df.shape)\n",
    "\n",
    "# 2 classes\n",
    "df2 = df.loc[(df['Pred']!= 2)& (df['True']!= 2)]\n",
    "y_true = df2['True']\n",
    "y_pred = df2['Pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cohort = 'External institute test'  # External institute test, Validation\n",
    "label_mapping = {0: 'DN', 1: 'HCC'}\n",
    "cm_outpath = f'geneSumscore/evaluation/cm_2{cohort}.pdf'\n",
    "\n",
    "R_CMAP = sns.light_palette('#D80000', as_cmap=True, n_colors=256)\n",
    "\n",
    "target_types = np.sort(pd.unique(y_true))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_percentage = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis],3)\n",
    "cm_df = pd.DataFrame(cm_percentage, index=target_types, columns=target_types)\n",
    "cm_df.columns = cm_df.columns.map(label_mapping)\n",
    "cm_df.index = cm_df.index.map(label_mapping)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))  # Set figure size for better visibility\n",
    "ax = sns.heatmap(cm_df, annot=True, fmt='.3f', square=True, \n",
    "                linewidths=2, linecolor='white', cmap=R_CMAP,  # Example cmap\n",
    "                cbar=False, xticklabels=True)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_title(f'{cohort} dataset')\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()  # Adjust layout to avoid clipping of labels\n",
    "plt.savefig(cm_outpath, bbox_inches='tight')\n",
    "plt.show()  # Optional: Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "R",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "56e847ae00843a4619a0cadd0bc464b4ab69b901b1c2b13ea68180fe453625d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
